"""Kal Tire scraper - Dynamic content extraction using Playwright."""
import json
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import re
from fuzzywuzzy import fuzz

from app.extractors.ocr.ocr_processor import ocr_image, detect_promo_keywords
from app.extractors.ocr.llm_cleaner import clean_promo_text_with_llm
from app.extractors.images.image_downloader import download_image, normalize_url
from app.config.constants import PROMO_KEYWORDS, DATA_DIR
from app.utils.logging_utils import setup_logger
from app.utils.promo_builder import build_standard_promo, load_existing_promos, apply_ai_overview_fallback, get_google_reviews_for_competitor

logger = setup_logger(__name__, "kal_scraper.log")

DATA_DIR.mkdir(parents=True, exist_ok=True)
PROMOTIONS_DIR = DATA_DIR / "promotions"
PROMOTIONS_DIR.mkdir(parents=True, exist_ok=True)


def extract_discount_value(text: str) -> Optional[str]:
    """Extract discount value from text."""
    text_lower = text.lower()

    # Try dollar amount first
    dollar_match = re.search(r'\$(\d+(?:\.\d+)?)', text)
    if dollar_match:
        return f"${dollar_match.group(1)}"

    # Try percentage
    percent_match = re.search(r'(\d+)\s*%', text)
    if percent_match:
        return f"{percent_match.group(1)}%"

    # Try "free"
    if "free" in text_lower:
        return "free"

    return None


def extract_brand_name(text: str) -> Optional[str]:
    """Extract brand name from text."""
    tire_brands = [
        "michelin", "bridgestone", "goodyear", "continental", "pirelli",
        "bfgoodrich", "bfgoodrich", "toyo", "nitto", "hankook", "falken",
        "kumho", "yokohama", "dunlop", "firestone", "general", "cooper",
        "uniroyal", "nexen", "hercules", "mastercraft", "goodrich"
    ]

    text_lower = text.lower()
    for brand in tire_brands:
        if brand in text_lower:
            return brand.title()
    return None


def extract_expiry_date(text: str) -> Optional[str]:
    """Extract expiry date from text."""
    date_patterns = [
        r'(?:expires?|expiry|valid until|until|ends?)[\s:]+(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
        r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})',
    ]
    for pattern in date_patterns:
        date_match = re.search(pattern, text, re.IGNORECASE)
        if date_match:
            return date_match.group(1).strip()
    return None


def calculate_title_overlap(title1: str, title2: str) -> float:
    """Calculate word overlap percentage between two titles."""
    if not title1 or not title2:
        return 0.0

    words1 = set(title1.lower().split())
    words2 = set(title2.lower().split())

    if not words1 or not words2:
        return 0.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    if not union:
        return 0.0

    return (len(intersection) / len(union)) * 100


def are_promos_duplicate(promo1: Dict, promo2: Dict) -> bool:
    """
    Check if two promotions are duplicates based on:
    1. Exact brand + discount match
    2. Same brand with discount within $5
    3. Same discount with 70%+ title word overlap
    """
    brand1 = promo1.get("brand_name") or extract_brand_name(promo1.get("promotion_title", ""))
    brand2 = promo2.get("brand_name") or extract_brand_name(promo2.get("promotion_title", ""))

    discount1 = promo1.get("discount_value", "")
    discount2 = promo2.get("discount_value", "")

    title1 = promo1.get("promotion_title", "")
    title2 = promo2.get("promotion_title", "")

    # Rule 1: Exact brand + discount match
    if brand1 and brand2 and brand1.lower() == brand2.lower():
        if discount1 and discount2 and discount1 == discount2:
            return True

        # Rule 2: Same brand with discount within $5
        if discount1 and discount2:
            # Extract numeric values
            val1_match = re.search(r'(\d+(?:\.\d+)?)', discount1)
            val2_match = re.search(r'(\d+(?:\.\d+)?)', discount2)
            if val1_match and val2_match:
                val1 = float(val1_match.group(1))
                val2 = float(val2_match.group(1))
                if abs(val1 - val2) <= 5:
                    return True

    # Rule 3: Same discount with 70%+ title word overlap AND same/unknown brand
    # Only consider duplicate if both have same discount AND high title overlap AND same brand
    if discount1 and discount2 and discount1 == discount2:
        overlap = calculate_title_overlap(title1, title2)
        # Only mark as duplicate if high overlap (85%+) AND brands match (or both unknown)
        brand_match = (brand1 and brand2 and brand1.lower() == brand2.lower()) or (not brand1 and not brand2)
        if overlap >= 85 and brand_match:
            return True

    return False


def keep_better_promo(promo1: Dict, promo2: Dict) -> Dict:
    """Keep the promotion with more complete information."""
    # Count non-empty fields
    fields1 = sum(1 for k, v in promo1.items() if v and k not in ["date_scraped", "ocr_hash"])
    fields2 = sum(1 for k, v in promo2.items() if v and k not in ["date_scraped", "ocr_hash"])

    # Prefer longer offer_details
    details1_len = len(promo1.get("offer_details", ""))
    details2_len = len(promo2.get("offer_details", ""))

    if fields1 > fields2 or (fields1 == fields2 and details1_len > details2_len):
        return promo1
    return promo2


def extract_card_content(card_element) -> Dict:
    """Extract content from a promo card element."""
    try:
        # Extract HTML text
        card_html = card_element.inner_html()
        card_text = card_element.inner_text()

        # Find images in the card
        images = card_element.query_selector_all("img")
        image_urls = []
        for img in images:
            src = img.get_attribute("src") or img.get_attribute("data-src")
            if src:
                image_urls.append(src)

        return {
            "html": card_html,
            "text": card_text,
            "image_urls": image_urls
        }
    except Exception as e:
        logger.error(f"Error extracting card content: {e}")
        return {"html": "", "text": "", "image_urls": []}


def process_kal_promotions(competitor: Dict) -> List[Dict]:
    """Process Kal Tire promotions using Playwright for dynamic content."""
    logger.info(f"Processing promotions for {competitor.get('name')}")

    promo_links = competitor.get("promo_links", [])
    if not promo_links:
        logger.warning(f"No promo_links found for {competitor.get('name')}")
        return []

    # Get Google Reviews once for this competitor
    google_reviews = get_google_reviews_for_competitor(competitor)

    from playwright.sync_api import sync_playwright

    all_promos = []

    try:
        with sync_playwright() as p:
            # Launch browser
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            page = context.new_page()

            promo_url = promo_links[0]
            logger.info(f"Loading page: {promo_url}")
            page.goto(promo_url, wait_until="networkidle", timeout=60000)

            # Wait for page to fully load
            page.wait_for_timeout(2000)

            # First, check if there are tabs - if so, click them
            # Tabs are typically in a tab list or navigation
            tabs_to_click = ["Tires", "Wheels", "Services"]

            # Check if tabs exist on the page
            has_tabs = False
            try:
                # Look for tab elements
                tab_elements = page.query_selector_all("[role='tab'], .tab, [class*='tab']")
                if len(tab_elements) > 0:
                    has_tabs = True
                    logger.info(f"Found {len(tab_elements)} tab elements")
            except:
                pass

            # Helper function to extract cards from current page state
            def extract_cards_from_current_view():
                card_selector = "div.rebates-item"
                cards = page.query_selector_all(card_selector)

                # If no cards found, try alternative selectors
                if len(cards) == 0:
                    alt_selectors = [
                        "div.kal-service-special-card",
                        "div[class*='rebate']",
                        "div[class*='promo']",
                        "div[class*='card']",
                        "div[class*='special']",
                    ]
                    for alt_sel in alt_selectors:
                        cards = page.query_selector_all(alt_sel)
                        if len(cards) > 0:
                            logger.info(f"Found cards using alternative selector: {alt_sel}")
                            card_selector = alt_sel
                            break
                return cards, card_selector

            # Find tab buttons within the rebates section
            tabs_wrapper = page.query_selector(".rebates-tabs-nav-wrapper")
            tab_names_to_click = ["Tires", "Wheels", "Services", "All", "View All", "See All"]  # Add more variations
            tabs_to_process = []  # Will collect tabs to process

            if tabs_wrapper:
                logger.info("Found tabs navigation, will click through each tab")
                # Get ALL tab buttons, not just specific ones
                tab_buttons = tabs_wrapper.query_selector_all("a, button, [role='button'], [role='tab'], .content-custom-btn")

                # Extract all unique tab names
                seen_tab_names = set()
                for tab_btn in tab_buttons:
                    try:
                        tab_text = tab_btn.inner_text().strip()
                        # Accept any tab that looks like a category tab (not too long, not empty)
                        if tab_text and len(tab_text) < 50 and tab_text not in seen_tab_names:
                            seen_tab_names.add(tab_text)
                            tabs_to_process.append(tab_text)
                            logger.info(f"Found tab: {tab_text}")
                    except Exception as e:
                        logger.debug(f"Error extracting tab text: {e}")
                        continue

                # If no tabs found but tabs_wrapper exists, try default view
                if not tabs_to_process:
                    logger.info("No tabs extracted, trying default view")
                    tabs_to_process = ["default"]
            else:
                logger.info("No tabs found, extracting from default view only")
                tabs_to_process = ["default"]

            # Process each tab and extract cards immediately (before DOM becomes stale)
            for tab_label in tabs_to_process:
                if tab_label != "default":
                    logger.info(f"Clicking tab: {tab_label}")
                    try:
                        # Try multiple selectors to find the tab
                        tab_element = None
                        selectors = [
                            f".rebates-tabs-nav-wrapper a:has-text('{tab_label}')",
                            f".rebates-tabs-nav-wrapper button:has-text('{tab_label}')",
                            f".rebates-tabs-nav-wrapper [role='tab']:has-text('{tab_label}')",
                            f"a:has-text('{tab_label}')",
                            f"button:has-text('{tab_label}')"
                        ]

                        for selector in selectors:
                            try:
                                tab_element = page.query_selector(selector)
                                if tab_element:
                                    break
                            except:
                                continue

                        if tab_element:
                            # Use JavaScript click to avoid navigation issues
                            page.evaluate("element => element.click()", tab_element)
                            page.wait_for_timeout(3000)  # Wait for content to load
                            logger.info(f"Successfully clicked tab '{tab_label}'")
                        else:
                            logger.warning(f"Tab '{tab_label}' not found with any selector, trying to extract from current view")
                            # Don't skip - try to extract from current view anyway
                    except Exception as e:
                        logger.warning(f"Error clicking tab '{tab_label}': {e}, continuing with current view")

                # Extract cards from current view IMMEDIATELY (while DOM is valid)
                cards, card_selector = extract_cards_from_current_view()
                logger.info(f"Found {len(cards)} promo cards in {tab_label} tab")

                # Process cards IMMEDIATELY while they're still valid in DOM
                for i, card in enumerate(cards):
                    logger.info(f"Processing card {i+1}/{len(cards)} from {tab_label}")

                    # Extract card content
                    card_data = extract_card_content(card)
                    card_text = card_data["text"]
                    card_html = card_data["html"]
                    image_urls = card_data["image_urls"]

                    if not card_text or len(card_text.strip()) < 10:
                        # Try OCR on images if text is missing
                        logger.info(f"Card text missing, trying OCR on images")
                        ocr_text_parts = []

                        for img_url in image_urls[:1]:  # Use first image only
                            try:
                                # Download and OCR
                                img_path = download_image(img_url)
                                if img_path:
                                    ocr_text = ocr_image(img_path)
                                    if ocr_text:
                                        ocr_text_parts.append(ocr_text)
                                    img_path.unlink()
                            except Exception as e:
                                logger.warning(f"OCR error for {img_url}: {e}")

                        if ocr_text_parts:
                            card_text = "\n".join(ocr_text_parts)
                        else:
                            logger.warning(f"No text or OCR content from card, skipping")
                            continue

                    # Skip if text is too short or doesn't contain promo keywords
                    is_promo = detect_promo_keywords(card_text, PROMO_KEYWORDS)
                    has_discount = extract_discount_value(card_text)
                    has_brand = extract_brand_name(card_text)
                    # Also check for common promo words that might not be in PROMO_KEYWORDS
                    has_promo_words = any(word in card_text.lower() for word in ["rebate", "save", "off", "special", "limited", "promo", "offer"])

                    if not is_promo and not (has_discount or has_brand or has_promo_words):
                        logger.info(f"Card doesn't contain promo keywords or relevant data, skipping")
                        continue

                    # Extract basic details
                    discount_value = extract_discount_value(card_text)
                    brand_name = extract_brand_name(card_text)
                    expiry_date = extract_expiry_date(card_text)

                    # Clean with LLM
                    context_text = f"Kal Tire promotion card from {tab_label} tab. HTML: {card_html[:500]}"
                    cleaned_data = clean_promo_text_with_llm(card_text, context_text)

                    # Build promotion title
                    if cleaned_data and cleaned_data.get("service_name"):
                        promotion_title = cleaned_data.get("service_name")
                    elif cleaned_data and cleaned_data.get("promo_description"):
                        first_line = cleaned_data.get("promo_description", "").split("\n")[0].strip()[:100]
                        promotion_title = first_line if first_line else card_text.split("\n")[0][:100]
                    else:
                        # Extract first meaningful line
                        lines = [l.strip() for l in card_text.split("\n") if l.strip() and len(l.strip()) > 5]
                        promotion_title = lines[0][:100] if lines else "Tire Promotion"

                    # Use LLM cleaned data if available
                    if cleaned_data:
                        service_name = cleaned_data.get("service_name", "tires")
                        promo_description = cleaned_data.get("promo_description", card_text[:500])
                        category = cleaned_data.get("category", "tires")
                        offer_details = cleaned_data.get("offer_details")
                        if not offer_details:
                            offer_parts = []
                            discount_val = cleaned_data.get("discount_value") or discount_value
                            coupon_code_val = cleaned_data.get("coupon_code")
                            expiry_date_val = cleaned_data.get("expiry_date") or expiry_date
                            if discount_val:
                                offer_parts.append(f"Discount: {discount_val}")
                            if coupon_code_val:
                                offer_parts.append(f"Code: {coupon_code_val}")
                            if expiry_date_val:
                                offer_parts.append(f"Expires: {expiry_date_val}")
                            if offer_parts:
                                offer_details = ". ".join(offer_parts) + ". " + card_text[:500]
                            else:
                                offer_details = card_text[:1000]
                    else:
                        service_name = "tires"
                        promo_description = card_text[:500]
                        category = "tires"
                        offer_parts = []
                        if discount_value:
                            offer_parts.append(f"Discount: {discount_value}")
                        if expiry_date:
                            offer_parts.append(f"Expires: {expiry_date}")
                        if offer_parts:
                            offer_details = ". ".join(offer_parts) + ". " + card_text[:500]
                        else:
                            offer_details = card_text[:1000]

                    # Load existing promos for comparison
                    output_file = PROMOTIONS_DIR / f"{competitor.get('name', 'kal').lower().replace(' ', '_')}.json"
                    existing_promos = load_existing_promos(output_file)
                    promo_key = f"{promo_url}::{service_name}"
                    existing_promo = existing_promos.get(promo_key)

                    promo = build_standard_promo(
                        competitor=competitor,
                        promo_url=promo_url,
                        service_name=service_name,
                        promo_description=promo_description,
                        category=category,
                        offer_details=offer_details,
                        ad_title=promotion_title,
                        ad_text=card_text[:200],
                        google_reviews=google_reviews,
                        existing_promo=existing_promo
                    )

                    all_promos.append(promo)
                    logger.info(f"[OK] Added promo: {promo.get('service_name', 'N/A')} - {promo.get('new_or_updated', 'NEW')}")

            browser.close()

    except Exception as e:
        logger.error(f"Error with Playwright: {e}", exc_info=True)

    # Deduplicate promotions
    logger.info(f"Found {len(all_promos)} promotions before deduplication")
    deduplicated_promos = []

    for promo in all_promos:
        is_duplicate = False

        for existing_promo in deduplicated_promos:
            if are_promos_duplicate(promo, existing_promo):
                # Keep the better one
                better_promo = keep_better_promo(promo, existing_promo)

                # Remove the worse one and add the better one
                if better_promo == promo:
                    deduplicated_promos.remove(existing_promo)
                    deduplicated_promos.append(promo)
                is_duplicate = True
                logger.info(f"Removed duplicate: {promo.get('promotion_title')} vs {existing_promo.get('promotion_title')}")
                break

        if not is_duplicate:
            deduplicated_promos.append(promo)

    logger.info(f"Total unique promotions found: {len(deduplicated_promos)}")
    return deduplicated_promos


def scrape_kal(competitor: Dict) -> Dict:
    """Main entry point for Kal Tire scraper."""
    from app.utils.extraction_flow import unified_extraction_flow, format_for_google_sheets

    try:
        # Use unified extraction flow
        promos = unified_extraction_flow(competitor, process_kal_promotions)

        # Format for Google Sheets
        formatted_promos = [format_for_google_sheets(promo) for promo in promos]

        # Save results
        output_file = PROMOTIONS_DIR / f"{competitor.get('name', 'kal').lower().replace(' ', '_')}.json"
        result = {
            "competitor": competitor.get("name"),
            "scraped_at": datetime.now().isoformat(),
            "promotions": formatted_promos,
            "count": len(formatted_promos)
        }

        output_file.write_text(json.dumps(result, indent=2, default=str))
        logger.info(f"Saved {len(formatted_promos)} promotions to {output_file}")

        return result

    except Exception as e:
        logger.error(f"Error scraping Kal Tire: {e}", exc_info=True)
        return {
            "competitor": competitor.get("name"),
            "error": str(e),
            "promotions": [],
            "count": 0
        }


if __name__ == "__main__":
    import sys
    from pathlib import Path

    # Load competitor data
    competitor_file = Path(__file__).parent.parent / "config" / "competitor_list.json"
    competitors = json.loads(competitor_file.read_text())

    # Find Kal Tire
    kal = next((c for c in competitors if "kal" in c.get("name", "").lower()), None)

    if not kal:
        logger.error("Kal Tire not found in competitor list")
        sys.exit(1)

    result = scrape_kal(kal)
    print(f"\nâœ… Scraping complete!")
    print(f"   Found {result.get('count', 0)} promotions")
    print(f"   Saved to: data/promotions/")
    print(f"\nðŸ“Š Summary:")
    for promo in result.get("promotions", []):
        print(f"   â€¢ {promo.get('promotion_title', 'N/A')}: {promo.get('discount_value', 'N/A')}")

